{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom LSTM Experiments Demo\n",
        "\n",
        "This notebook demonstrates two projects built with a custom LSTM cell implementation:\n",
        "1. **Binary Counting** — learning to count ones in binary sequences.\n",
        "2. **Sine Wave Prediction** — learning to predict the next values in a sine wave sequence.\n",
        "\n",
        "We'll explore training, evaluation, and visualization for both projects.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup and Imports\n"
      ],
      "metadata": {
        "id": "NlZOBW-aQGk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import copy\n",
        "\n",
        "from lstm_cell import LSTMCell\n",
        "from binary_counting import LSTMModel, train_model\n",
        "from sine_wave_prediction import LSTMModelTrig, input_data\n"
      ],
      "metadata": {
        "id": "TBdH9-UGQIfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Binary Counting Task\n",
        "\n",
        "In this project, our custom LSTM model learns to count the number of ones in a binary sequence.\n",
        "\n",
        "### Dataset\n",
        "Random binary sequences of length 20; targets are the sum of ones in each sequence.\n",
        "\n",
        "### Model\n",
        "Custom LSTM cell + fully connected output layer.\n"
      ],
      "metadata": {
        "id": "cFkvvuVdQTF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "EPOCHS = 10\n",
        "TRAINING_SAMPLES = 2000\n",
        "BATCH_SIZE = 32\n",
        "TEST_SAMPLES = 500\n",
        "SEQUENCE_LENGTH = 20\n",
        "HIDDEN_UNITS = 20\n",
        "\n",
        "# Data\n",
        "X_train = torch.randint(0, 2, (TRAINING_SAMPLES, SEQUENCE_LENGTH, 1)).float()\n",
        "X_test = torch.randint(0, 2, (TEST_SAMPLES, SEQUENCE_LENGTH, 1)).float()\n",
        "y_train = torch.sum(X_train, dim=1)\n",
        "y_test = torch.sum(X_test, dim=1)\n",
        "\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model and training setup\n",
        "model = LSTMModel(input_size=1, hidden_size=HIDDEN_UNITS, output_dim=1)\n",
        "model_untrained = copy.deepcopy(model)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Train\n",
        "train_loss, train_acc, valid_loss, valid_acc = train_model(\n",
        "    model, loss_fn, optimizer, train_dl, test_dl, tolerance=1\n",
        ")\n",
        "\n",
        "# Visualization: Before and After Training\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "y_pred_before = model_untrained(X_test).detach().numpy().round()\n",
        "plt.scatter(y_test.numpy(), y_pred_before, alpha=0.3)\n",
        "plt.title(\"Before Training\")\n",
        "plt.xlabel(\"True Sum\")\n",
        "plt.ylabel(\"Predicted Sum\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "y_pred_after = model(X_test).detach().numpy().round()\n",
        "plt.scatter(y_test.numpy(), y_pred_after, alpha=0.3)\n",
        "plt.title(\"After Training\")\n",
        "plt.xlabel(\"True Sum\")\n",
        "plt.ylabel(\"Predicted Sum\")\n",
        "\n",
        "plt.suptitle(\"Binary Counting Task\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tuT18tDHQU0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Sine Wave Prediction Task\n",
        "\n",
        "Here, we train a custom LSTM model to predict the next values of a sine wave sequence using a sliding window approach.\n",
        "\n",
        "### Dataset\n",
        "Sine wave values sampled over 800 points; last 40 points held out for testing.\n",
        "\n",
        "### Model\n",
        "Custom LSTM cell + fully connected output layer.\n"
      ],
      "metadata": {
        "id": "B7JPyo0qQqk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(20)\n",
        "window_size = 40\n",
        "num_epochs = 5\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Prepare data\n",
        "x = torch.linspace(0, 799, 800)\n",
        "y = torch.sin(x * torch.pi * 2 / 40)\n",
        "y_train, y_test = y[:-40], y[-40:]\n",
        "train_data = input_data(y_train, window_size)\n",
        "\n",
        "# Model setup\n",
        "model = LSTMModelTrig(input_size=1, hidden_size=50)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    for x_batch, y_batch in train_data:\n",
        "        x_batch = x_batch.view(1, window_size, 1)\n",
        "        y_pred = model(x_batch)[0]\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_data)\n",
        "\n",
        "    # Predict last window_size points\n",
        "    sample_input = y_train[-window_size:].tolist()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(window_size):\n",
        "            input_tensor = torch.tensor(sample_input[-window_size:]).view(1, window_size, 1)\n",
        "            pred = model(input_tensor).item()\n",
        "            sample_input.append(pred)\n",
        "\n",
        "    test_loss = loss_fn(torch.tensor(sample_input[-window_size:]), y_train[-window_size:])\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | {time.time() - start_time:.2f} sec | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Visualization of predictions\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(y.numpy(), color=\"#8000ff\", label=\"True Signal\")\n",
        "plt.plot(range(760, 800), sample_input[-window_size:], color=\"#ff8000\", label=\"Predicted\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.title(\"Sine Wave Prediction Task\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"sin(x)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vj_xJLRoQsKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "- The **Binary Counting model** effectively learns to count the number of ones in binary sequences, as shown by the improved alignment of predicted vs. true sums after training.\n",
        "- The **Sine Wave Prediction model** successfully predicts the future values of the sine wave based on past inputs, capturing the periodicity well.\n",
        "- Both models use the same custom LSTM cell implementation, demonstrating the flexibility and power of custom recurrent units in PyTorch.\n",
        "- This notebook can serve as a foundation for experimenting with custom RNN/LSTM cells on various sequence tasks.\n",
        "\n",
        "Feel free to explore and modify the models and training parameters to deepen your understanding!\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "iZkKzpdNQwLU"
      }
    }
  ]
}