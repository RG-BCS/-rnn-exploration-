{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with LSTM on *The Mysterious Island*\n",
        "\n",
        "This notebook demonstrates training a character-level LSTM model to generate text inspired by *The Mysterious Island* novel (Project Gutenberg).  \n",
        "We cover data loading, preprocessing, model definition, training, and text generation.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup & Imports\n"
      ],
      "metadata": {
        "id": "D1ABx9SCBBqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-YO72v2bBEYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Download and Preprocess Dataset\n",
        "\n",
        "We download the text, extract the main content, and create character-level encodings for model input.\n"
      ],
      "metadata": {
        "id": "wfBOtSbYBLbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "SEED = 10\n",
        "SEQUENCE_LENGTH = 40\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Download text\n",
        "!wget -q https://www.gutenberg.org/files/1268/1268-0.txt\n",
        "\n",
        "with open(\"1268-0.txt\", 'r', encoding='utf8') as fp:\n",
        "    text = fp.read()\n",
        "\n",
        "# Extract main content between known markers\n",
        "start_index = text.find(\"THE MYSTERIOUS ISLAND\")\n",
        "end_index = text.find(\"END OF THE PROJECT GUTENBERG EBOOK 1268\")\n",
        "text = text[start_index:end_index]\n",
        "\n",
        "# Create character mappings\n",
        "char_set = set(text)\n",
        "char_sorted = sorted(char_set)\n",
        "char2int = {ch: i for i, ch in enumerate(char_sorted)}\n",
        "char_array = np.array(char_sorted)\n",
        "\n",
        "# Encode entire text to integer sequence\n",
        "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
        "\n",
        "print(f\"Total characters in text: {len(text)}\")\n",
        "print(f\"Unique characters (vocabulary size): {len(char2int)}\")\n"
      ],
      "metadata": {
        "id": "LIwOW2OyBTj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Create Dataset and DataLoaders\n",
        "\n",
        "Split the encoded text into sequences and create PyTorch Dataset and DataLoader objects for training and validation.\n"
      ],
      "metadata": {
        "id": "sVJ-3zgMBXhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        chunk = self.text_chunks[index]\n",
        "        return torch.tensor(chunk[:-1], dtype=torch.long), torch.tensor(chunk[1:], dtype=torch.long)\n",
        "\n",
        "chunk_size = SEQUENCE_LENGTH + 1\n",
        "text_chunks = [text_encoded[i:chunk_size + i] for i in range(len(text_encoded) - chunk_size + 1)]\n",
        "\n",
        "dataset = TextDataset(text_chunks)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_ds, valid_ds = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "print(f\"Training batches: {len(train_dl)}\")\n",
        "print(f\"Validation batches: {len(valid_dl)}\")\n"
      ],
      "metadata": {
        "id": "UiBHh4UOBabh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Define the LSTM Model\n",
        "\n",
        "We define a character-level LSTM model with embedding, LSTM layers, and a fully connected output layer.\n"
      ],
      "metadata": {
        "id": "O4NbxUuVBdkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCharGenModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "2ktl2-kWBgea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 5. Define Training and Utility Functions\n",
        "\n",
        "These include gradient norm calculation, training loop, and text generation functions.\n"
      ],
      "metadata": {
        "id": "ebaE92ksBjWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def grad_norm(model):\n",
        "    total_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    return total_norm ** 0.5\n",
        "\n",
        "def train_model(model, loss_fn, optimizer, train_dl, valid_dl, num_epochs, sequence_length, device):\n",
        "    train_losses = []\n",
        "\n",
        "    def evaluate(x_batch, y_batch, train=True):\n",
        "        model.train() if train else model.eval()\n",
        "        batch_size = x_batch.size(0)\n",
        "        hidden, cell = model.init_hidden(batch_size)\n",
        "        hidden, cell = hidden.to(device), cell.to(device)\n",
        "        loss = 0.0\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        for c in range(sequence_length):\n",
        "            input_ = x_batch[:, c].to(device)\n",
        "            pred, hidden, cell = model(input_, hidden, cell)\n",
        "            hidden = hidden.detach()\n",
        "            cell = cell.detach()\n",
        "            y_batch_step = y_batch[:, c].to(device)\n",
        "            loss += loss_fn(pred, y_batch_step)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        return loss.item() / sequence_length\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        train_epoch_loss = 0.0\n",
        "        for x_batch, y_batch in train_dl:\n",
        "            train_epoch_loss += evaluate(x_batch, y_batch, train=True)\n",
        "        train_losses.append(train_epoch_loss)\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "            valid_epoch_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in valid_dl:\n",
        "                    valid_epoch_loss += evaluate(x_batch, y_batch, train=False)\n",
        "\n",
        "            train_epoch_loss /= len(train_dl)\n",
        "            valid_epoch_loss /= len(valid_dl)\n",
        "            elapsed = time.time() - start\n",
        "            norm_grad = grad_norm(model)\n",
        "            print(f'Epoch {epoch}/{num_epochs} | time_elapsed: {elapsed:.3f}s | train_loss: {train_epoch_loss:.4f} | '\n",
        "                  f'valid_loss: {valid_epoch_loss:.4f} | grad_norm: {norm_grad:.4f}')\n",
        "    return train_losses\n",
        "\n",
        "def generate_text(model, text, char_length, char2int, char_array, device):\n",
        "    tokens = torch.tensor([char2int[c] for c in text]).to(device)\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden, cell = hidden.to(device), cell.to(device)\n",
        "    for i in range(len(tokens)):\n",
        "        input_ = tokens[i:i+1]\n",
        "        out, hidden, cell = model(input_, hidden, cell)\n",
        "    for _ in range(char_length):\n",
        "        y_pred = torch.argmax(out, dim=1).item()\n",
        "        text += char_array[y_pred]\n",
        "        input_ = torch.tensor([y_pred]).to(device)\n",
        "        out, hidden, cell = model(input_, hidden, cell)\n",
        "    return text\n",
        "\n",
        "def random_next_text(model, text, char_length, scale_factor, char2int, char_array, device):\n",
        "    model.eval()\n",
        "    for _ in range(char_length):\n",
        "        tokens = torch.tensor([char2int[c] for c in text]).to(device)\n",
        "        hidden, cell = model.init_hidden(1)\n",
        "        hidden, cell = hidden.to(device), cell.to(device)\n",
        "        for i in range(len(tokens)):\n",
        "            input_ = tokens[i:i+1]\n",
        "            out, hidden, cell = model(input_, hidden, cell)\n",
        "        m = Categorical(logits=out * scale_factor)\n",
        "        y_pred = m.sample((1,)).item()\n",
        "        text += char_array[y_pred]\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "H7xAZURrBnxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6. Initialize Model and Train\n",
        "\n",
        "Set hyperparameters, instantiate the model, loss, optimizer, and start training.\n"
      ],
      "metadata": {
        "id": "splGZldJBqCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 50\n",
        "VOCAB_SIZE = len(char_array)\n",
        "EMBED_DIM = 256\n",
        "RNN_HIDDEN_SIZE = 512\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "model = RNNCharGenModel(VOCAB_SIZE, EMBED_DIM, RNN_HIDDEN_SIZE).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loss = train_model(model, loss_fn, optimizer, train_dl, valid_dl,\n",
        "                         NUM_EPOCHS, SEQUENCE_LENGTH, device)\n"
      ],
      "metadata": {
        "id": "lHH5NMknBy_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 7. Generate Text Samples\n",
        "\n",
        "We generate text using greedy decoding and random sampling with different temperatures.\n"
      ],
      "metadata": {
        "id": "3czcAaPYB2oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_initiator = \"The island\"\n",
        "\n",
        "print(\"1. Greedy (argmax) text generation:\\n\")\n",
        "print(generate_text(model, text_initiator, 500, char2int, char_array, device))\n",
        "print('#' * 80)\n",
        "\n",
        "print(\"2. Random sampling with temperature = 1.0:\\n\")\n",
        "print(random_next_text(model, text_initiator, 500, 1.0, char2int, char_array, device))\n",
        "print('#' * 80)\n",
        "\n",
        "print(\"3. Random sampling with temperature = 2.0:\\n\")\n",
        "print(random_next_text(model, text_initiator, 500, 2.0, char2int, char_array, device))\n",
        "print('#' * 80)\n",
        "\n",
        "print(\"4. Random sampling with temperature = 0.5:\\n\")\n",
        "print(random_next_text(model, text_initiator, 500, 0.5, char2int, char_array, device))\n",
        "print('#' * 80)\n"
      ],
      "metadata": {
        "id": "2lFXABJoB4fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 8. Plot Training Loss\n",
        "\n",
        "Visualizing the training loss across epochs to monitor convergence.\n"
      ],
      "metadata": {
        "id": "jt-Yi_uoB777"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss)\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F4uN-O_aB-hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "We successfully trained a character-level LSTM on *The Mysterious Island* text. The model learned to generate coherent text sequences mimicking the style and structure of the novel.  \n",
        "\n",
        "Different sampling temperatures in random text generation affect creativity vs. coherence tradeoffs. Lower temperatures yield more predictable text, while higher temperatures introduce more randomness.\n",
        "\n",
        "This notebook can be extended by:\n",
        "\n",
        "- Increasing model complexity or layers  \n",
        "- Training for more epochs  \n",
        "- Experimenting with word-level models  \n",
        "- Adding attention mechanisms  \n",
        "\n",
        "Feel free to explore and improve!\n",
        "\n",
        "---\n",
        "\n",
        "*Happy Text Generating!*\n"
      ],
      "metadata": {
        "id": "A2WSjZR-CDyr"
      }
    }
  ]
}