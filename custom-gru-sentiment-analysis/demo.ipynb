{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Using Custom and Built-in GRU Models\n",
        "\n",
        "This notebook demonstrates sentiment analysis on the IMDB movie reviews dataset using three different GRU architectures:\n",
        "\n",
        "1. **Custom GRU** — a manually implemented GRU cell and model from scratch  \n",
        "2. **Uni-directional GRU** — PyTorch's built-in GRU moving left-to-right  \n",
        "3. **Bi-directional GRU** — PyTorch's built-in GRU capturing context in both directions  \n",
        "\n",
        "We compare the training performance and evaluation metrics of these models to understand their strengths and weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "## Why GRUs?\n",
        "\n",
        "Gated Recurrent Units (GRUs) are a popular type of RNN that efficiently capture dependencies in sequence data with fewer parameters than LSTMs. They help handle the vanishing gradient problem in RNNs, making them suitable for NLP tasks like sentiment classification.\n"
      ],
      "metadata": {
        "id": "wx8zELKq26ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Preprocessing\n",
        "\n",
        "We use the IMDB dataset with labeled movie reviews as positive or negative.\n",
        "\n",
        "Key preprocessing steps include:\n",
        "\n",
        "- Tokenizing text into words (excluding punctuation)\n",
        "- Building a vocabulary based on token frequency\n",
        "- Mapping tokens and labels to integer indices\n",
        "- Padding sequences within each batch for uniform input length\n",
        "\n",
        "The data is then loaded into PyTorch DataLoaders for efficient mini-batch training.\n"
      ],
      "metadata": {
        "id": "7YGIbT9_3gP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataloader_generator import get_dataloaders\n",
        "from models import GRU_Sentiment_Analysis, GRULeftToRight, BidirectionalGRU\n",
        "from utils import train_each_models, plot_confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DATA_PATH = \"hf://datasets/scikit-learn/imdb/IMDB Dataset.csv\"\n",
        "train_dl, valid_dl, vocab = get_dataloaders(DATA_PATH)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n"
      ],
      "metadata": {
        "id": "nrF3CTUm34Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Helper\n",
        "\n",
        "We define a helper function to:\n",
        "\n",
        "- Initialize models with fixed seeds for reproducibility  \n",
        "- Train models for a specified number of epochs  \n",
        "- Plot training and validation accuracy and loss curves  \n",
        "- Display the confusion matrix on the validation set\n",
        "\n",
        "This modularity allows easy comparison of different architectures.\n"
      ],
      "metadata": {
        "id": "JM1KmDOP4Fr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(model_class, seed, model_name):\n",
        "    torch.manual_seed(seed)\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    model = model_class(vocab_size, 20, 64, 64).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    train_acc, train_loss, valid_acc, valid_loss = train_each_models(\n",
        "        model, loss_fn, optimizer, train_dl, valid_dl, device=device, NUM_EPOCHS=10\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_acc, label=\"Train Accuracy\")\n",
        "    plt.plot(valid_acc, label=\"Validation Accuracy\")\n",
        "    plt.title(f\"{model_name} Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_loss, label=\"Train Loss\")\n",
        "    plt.plot(valid_loss, label=\"Validation Loss\")\n",
        "    plt.title(f\"{model_name} Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plot_confusion_matrix(model, valid_dl, device=device, title=f\"{model_name} Confusion Matrix\")\n"
      ],
      "metadata": {
        "id": "ulKRqAW_4H_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom GRU Model Training\n",
        "\n",
        "First, we train our manually implemented GRU model.  \n",
        "This gives insight into the inner workings of GRUs and serves as a baseline.\n"
      ],
      "metadata": {
        "id": "AFVnfMOA4Qug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(GRU_Sentiment_Analysis, seed=1, model_name=\"Custom GRU\")"
      ],
      "metadata": {
        "id": "JNa3-ikd4SJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uni-directional GRU Training\n",
        "\n",
        "Next, we train a standard left-to-right GRU using PyTorch's built-in implementation.\n",
        "\n",
        "This model usually achieves better performance than the custom GRU due to optimized internals.\n"
      ],
      "metadata": {
        "id": "IrYFvqad4axg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(GRULeftToRight, seed=2, model_name=\"Uni-directional GRU\")\n"
      ],
      "metadata": {
        "id": "I5xV1aZd4cXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-directional GRU Training\n",
        "\n",
        "Finally, we train a bidirectional GRU to leverage both past and future context.\n",
        "\n",
        "This typically yields the best performance on sequence classification tasks like sentiment analysis.\n"
      ],
      "metadata": {
        "id": "Ep77Bxjt4fKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(BidirectionalGRU, seed=3, model_name=\"Bi-directional GRU\")"
      ],
      "metadata": {
        "id": "QPIseY_F4ja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary and Conclusion\n",
        "\n",
        "- The **Custom GRU** implementation serves as an educational baseline, showing how GRU cells can be built from scratch. However, it generally trains slower and performs slightly worse.\n",
        "- The **Uni-directional GRU** uses optimized PyTorch components and improves accuracy by efficiently capturing sequential dependencies from left to right.\n",
        "- The **Bi-directional GRU** leverages context from both past and future tokens, achieving the highest accuracy and most robust results.\n",
        "- Bidirectional RNNs are particularly effective in NLP tasks where understanding both preceding and succeeding words enhances comprehension.\n",
        "- Implementing and comparing these models provides both practical experience and theoretical insight into sequence modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Experiment with hyperparameter tuning (embedding sizes, hidden units, learning rates)\n",
        "- Extend to multi-class sentiment classification or other NLP tasks  \n",
        "- Explore advanced architectures like attention mechanisms or Transformers for state-of-the-art results\n"
      ],
      "metadata": {
        "id": "5YhVEFft4mb-"
      }
    }
  ]
}